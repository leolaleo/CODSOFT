# -*- coding: utf-8 -*-
"""Movie_genre_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MokIlSKOSbmpvlp1rUWW-frv3rIBcwct
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from wordcloud import WordCloud

import nltk
import os

custom_download_path = r'C:\Users\ABHIRAMI.K\Documents\Movie genre classification\nltk_data'

# Download specific NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

"""1. Data Collection"""

# Read the txt files using pandas

  train_data = pd.read_csv(r'/content/train_data.txt', delimiter=':::' ,header = None ,engine='python')
  test_data  = pd.read_csv(r'/content/test_data.txt', delimiter=':::' ,header = None ,engine='python')

test_data_solution  = pd.read_csv(r'/content/test_data_solution.txt', delimiter=':::' ,header = None ,engine='python')

## View train data
print("shape",train_data.shape)
train_data.head()

## View the test solution data
print("shape",test_data_solution.shape)
test_data_solution.head()

## will concat the test and train file

df = pd.concat((train_data ,test_data_solution))
df.columns = ["id" ,"Title","Genre","Description"]
df.head()

## Check the size
df.shape

"""2. Data Cleaning and Preprocessing"""

## Check for Duplicates and Remove them
df.duplicated().sum() ## Will give us a number of duplicates

df.drop_duplicates(inplace = True)  ## Will drops any duplicates

## Check for nan values

df.isna().sum()  # Will check for any duplicates

df.dropna( inplace = True ) ## Will drop any nan containing row if exists

## Check the size
df.shape

## function to preprocess the data
stopword = set(stopwords.words('english'))

def preprocessing(text):
    # Convert text to lowercase
    text = text.lower()

    # Remove punctuation using regular expressions
    text = re.sub(r'[^\w\s]', '', text)

    # Remove specific characters #, @, and $
    text = re.sub(r'[#@\$]', '', text)

    # tokenize and convert to list
    tokens = word_tokenize(text)

    ## Lemmatize it
    lemmatizer  = WordNetLemmatizer()

    ## lemmatize each token
   # text = [lemmatizer.lemmatize(token) for token in tokens]
    text = text.split()

    text = [word for word in text if word not in stopword]


    return " ".join(text)

## Create list of words in discription column
df["Despcription_clean"] =  df["Description"].apply(preprocessing)

df.head()

"""3. Data Visualization"""

## Shows us the label counts
df["Genre"].value_counts()

# Create a histogram of genre distribution
plt.figure(figsize=(20, 10))
plt.hist(df["Genre"],bins =27 , color='blue', alpha=0.7)
plt.title("Genre Distribution")
plt.xlabel("Genres")
plt.ylabel("Frequency")
plt.xticks(rotation=90)
plt.show()

## View genre distribution on Horizontal graph

genre_counts = df["Genre"].value_counts()
sorted_genres = genre_counts.sort_values(ascending=True)
# Create a horizontal histogram of genre distribution
plt.figure(figsize=(10, 15))
sorted_genres.plot(kind='barh',color = "blue", alpha=1 )
plt.title("Genre Distribution")
plt.xlabel("Frequency")
plt.ylabel("Genres")
plt.show()

"""4. Feature Engineering"""

## remove id column from head
data = df.drop(["Title","id"] , axis = 1) # will drop column
data.head()

"""5. Model Selection & Training"""

from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

"""Converting Genre into Numerical form"""

#Convert sentiment labels to numerical values for modeling
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
data['Genre_encoded'] = label_encoder.fit_transform(data['Genre'])
data['Genre_encoded']

class_names= list(label_encoder.classes_)
class_names

data.head()

"""Split the data to test and train"""

## Split the data
x = data["Despcription_clean"]
y = data["Genre"]

x_train ,x_test ,y_train ,y_test = train_test_split(x ,y ,test_size = 0.5)

"""Model training using TD-IDF technique"""

vectorizer = TfidfVectorizer()
x_train1 = vectorizer.fit_transform(x_train)
x_test1 = vectorizer.transform(x_test)

"""LogisticRegression"""

## select Logistic regression for this
model = LogisticRegression()
model.fit(x_train1 ,y_train)
print("Model Score on Training data",model.score(x_train1 ,y_train))
print("Model Score on Training data",model.score(x_test1 ,y_test))
y_pred = model.predict(x_test1)
print(classification_report(y_pred ,y_test))


cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(15, 15))  # Adjust the figure size as needed
sns.heatmap(cm, annot=True, fmt='d', cbar=False,
            xticklabels=class_names, yticklabels=class_names)  # Replace 'class_names' with your class labels
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""SVM"""

from sklearn.svm import LinearSVC

# Select SVC model (LinearSVC)
svm = LinearSVC()
svm.fit(x_train1, y_train)
print("Model Score on Training data:", svm.score(x_train1, y_train))
print("Model Score on Test data:", svm.score(x_test1, y_test))
y_pred = svm.predict(x_test1)
print(classification_report(y_pred, y_test))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Plotting Confusion Matrix
plt.figure(figsize=(15, 15))
sns.heatmap(cm, annot=True, fmt='d', cbar=False,
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix Heatmap')
plt.show()

